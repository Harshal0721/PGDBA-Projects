# -*- coding: utf-8 -*-
"""Gallstone_Prediction_final.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1rdFNWUyKATFA_oH2ELhRCLWIB6Ls_Ns2

# Early Prediction of Gallstone using Statistical and Machine Learning Techniques

Submitted by


*  Arunava Mukherjee (24BM6JP11)
*  Prabuddha Durge(24BM6JP17)
*  Harshal Bhagwat Ugalmugle (24BM6JP20)
*  Hemraj Chakravarti (24BM6JP22)

**Techniques Used**
1. Dependency Relationship between Features and Target Variable *(binary)* using Mutual Information
2. Statistical Tests for Independence using Chi-Square test of Independence for discrete variables
3. Statistical Tests for Independence using one-way F test for continuous variables
4. Portmanteau Test using Logistic Regression and Likelihood Ratio Test for joint-hypothesis testing
5. Modelling Gallstone Prediction using Logistic Regression, Decision Trees, Random Forests, XGBoost (with dropout)
6. Best Performance in terms of accuracy obtained with XGBoost of 85.94 which is a 0.52 pts improvement over the original paper
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split
from sklearn.feature_selection import mutual_info_classif
from scipy.stats import chi2_contingency, f_oneway
from scipy.stats import mannwhitneyu
import statsmodels.api as sm
from scipy.stats import chi2
from sklearn.metrics import roc_auc_score, roc_curve, confusion_matrix, ConfusionMatrixDisplay

"""### Data Loading"""

df=pd.read_csv('Gallstone_Prediction_Dataset.csv')

df

df['Gallstone_Status'] = 1 - df['Gallstone Status']
df.drop('Gallstone Status', axis=1, inplace=True)

df

"""### Train-Test Split"""

df_train, df_test = train_test_split(df,test_size=0.2,random_state=42)

for cols in df_train.columns:
    print(f"==============UNIQUE VALUES IN COL: {cols}====================")
    print(df_train[cols].nunique())
    print("===============================================================")

cat_cols = [col for col in df_train.columns if df_train[col].nunique() < 5 and col != 'Gallstone_Status']
len(cat_cols)

fig, axes = plt.subplots(4, 2, figsize=(12, 16))
axes = axes.flatten()

for ax, col in zip(axes, cat_cols):
    sns.countplot(
        x=col,
        hue='Gallstone_Status',
        data=df_train,
        palette='Set1',
        ax=ax
    )
    ax.set_title(f'Countplot of {col} by Gallstone Status')
    ax.set_xlabel(col)
    ax.set_ylabel('Count')
    ax.legend(title='Gallstone Status')

# If there are unused subplots (e.g., fewer than 6 columns), remove them
for ax in axes[len(cat_cols):]:
    fig.delaxes(ax)

plt.tight_layout()
plt.show()

num_cols=[col for col in df_train.columns if col not in cat_cols and col != 'Gallstone_Status']
len(num_cols)

fig, axes = plt.subplots(8, 5, figsize=(12, 24))
axes = axes.flatten()

for ax, col in zip(axes, num_cols):
    sns.boxplot(
        x='Gallstone_Status',
        y=col,
        data=df_train,
        hue='Gallstone_Status',
        palette='Set1',
        ax=ax,
        notch=True
    )
    #ax.set_title(f'{col}')
    ax.set_xlabel('Gallstone Status')
    ax.set_ylabel(col)
    # Remove legend from individual plots
    if ax.get_legend():
        ax.get_legend().remove()

# Remove any unused subplots
for ax in axes[len(num_cols):]:
    fig.delaxes(ax)

plt.tight_layout()
plt.show()

"""### Data Visualization"""

# Compute the correlation matrix
correlation_matrix = df_train[num_cols].corr()

# Set up the matplotlib figure
plt.figure(figsize=(12, 8))

# Create the heatmap
sns.heatmap(
    correlation_matrix,
    fmt=".2f",
    cmap='coolwarm',
    cbar=True,
    square=True
)

# Set title
plt.title('Correlation Heatmap for Numeric Columns in num_cols')
plt.show()

# Compute Mutual Information (MI) scores
X = df_train.drop(columns=['Gallstone_Status'])
y = df_train['Gallstone_Status']
mi_scores = mutual_info_classif(X, y, discrete_features='auto', random_state=42)

# Create a DataFrame for MI scores
mi_df = pd.DataFrame({
    'Feature': X.columns,
    'MI Score': mi_scores
}).sort_values(by='MI Score', ascending=False)

# Plot the MI scores
plt.figure(figsize=(12, 6))
sns.barplot(
    x='Feature',
    y='MI Score',
    data=mi_df,
    palette='viridis',
    hue='MI Score'
)

# Rotate x-axis labels for clarity
plt.xticks(rotation=45, ha='right')

# Set labels and title
plt.xlabel('Feature Name')
plt.ylabel('Mutual Information (MI) Score')
plt.title('Mutual Information Scores for Features with Respect to Gallstone Status')

plt.tight_layout()
plt.show()

mi_df

mi_df = mi_df[mi_df['MI Score'] > 0]

excluded_columns = [col for col in df_train.columns if col not in mi_df['Feature'].tolist() and col != 'Gallstone_Status']

excluded_columns

mi_df

for feature in mi_df['Feature']:
    if feature in cat_cols:  # If the feature is categorical
        # Generate the contingency table
        contingency_table = pd.crosstab(df_train[feature], df_train['Gallstone_Status'])
        # Perform Chi-squared test
        chi2, p, _, _ = chi2_contingency(contingency_table)
        # Determine statistical significance
        mi_df.loc[mi_df['Feature'] == feature, 'Status'] = 'Statistically Significant' if p < 0.05 else 'Statistically Insignificant'

    elif feature in num_cols:  # If the feature is numerical
        # Perform Mann-Whitney U test
        group_0 = df_train[df_train['Gallstone_Status'] == 0][feature]
        group_1 = df_train[df_train['Gallstone_Status'] == 1][feature]

        # Ensure the two groups have at least one sample for comparison
        if len(group_0) > 0 and len(group_1) > 0:
            u_stat, p = mannwhitneyu(group_0, group_1, alternative='two-sided')
            # Determine statistical significance
            mi_df.loc[mi_df['Feature'] == feature, 'Status'] = 'Statistically Significant' if p < 0.05 else 'Statistically Insignificant'
        else:
            # If one of the groups has no data, mark as insufficient for statistical testing
            mi_df.loc[mi_df['Feature'] == feature, 'Status'] = 'Insufficient Data'

# Display updated mi_df
mi_df

for feature in mi_df['Feature']:
    if feature in cat_cols:  # If the feature is categorical
        # Generate and display the contingency table
        contingency_table = pd.crosstab(df_train[feature], df_train['Gallstone_Status'])
        print(f"\nContingency Table for {feature} vs Gallstone_Status:")
        print(contingency_table)

        # Perform Chi-squared test
        chi2, p, _, _ = chi2_contingency(contingency_table)
        print(f"Chi-squared test for {feature}: Chi2={chi2:.4f}, p-value={p:.4f}")

        # Optional: Plot the contingency table as a heatmap for visualization
        plt.figure(figsize=(8, 6))
        sns.heatmap(contingency_table, annot=True, cmap="Blues", fmt='d')
        plt.title(f'Contingency Table Heatmap for {feature} and Gallstone Status')
        plt.xlabel('Gallstone Status')
        plt.ylabel(feature)
        plt.show()

cat_cols

from scipy.stats import fisher_exact

# Define the conditions to test
conditions_to_test = ['Coronary Artery Disease (CAD)', 'Hyperlipidemia', 'Hypothyroidism']

# Perform Fisher's Exact Test for each condition
for condition in conditions_to_test:
    if condition in df_train.columns:
        # Create a contingency table for the current condition
        contingency_table = pd.crosstab(df_train[condition], df_train['Gallstone_Status'])

        # Ensure the contingency table has 2x2 dimensions for Fisher's Exact Test
        # If a category for the condition or Gallstone_Status is missing, we need to handle it.
        # Pad the table with zeros if a category is missing.
        # Expected categories for a binary variable: 0 and 1
        condition_levels = [0, 1]
        gallstone_levels = [0, 1]
        full_contingency = pd.DataFrame(np.zeros((len(condition_levels), len(gallstone_levels))),
                                        index=condition_levels, columns=gallstone_levels)
        # Update the full_contingency table with the actual counts
        for cond_level in contingency_table.index:
            for gall_level in contingency_table.columns:
                if cond_level in full_contingency.index and gall_level in full_contingency.columns:
                    full_contingency.loc[cond_level, gall_level] = contingency_table.loc[cond_level, gall_level]

        print(f"\nFisher's Exact Test for {condition}:")
        print("Contingency Table:")
        print(full_contingency)

        # Perform Fisher's Exact Test
        # We pass the table values as a list of lists
        table_values = full_contingency.values.tolist()
        if np.sum(table_values) > 0 and full_contingency.shape == (2, 2):
            odds_ratio, p_value = fisher_exact(table_values)

            print(f"Odds Ratio: {odds_ratio:.4f}, p-value: {p_value:.4f}")

            # Decide significance (using a common alpha level of 0.05)
            if p_value < 0.05:
                print(f"Conclusion: There is a statistically significant association between {condition} and Gallstone Status.")
            else:
                print(f"Conclusion: There is no statistically significant association between {condition} and Gallstone Status.")
        else:
            print(f"Insufficient or malformed data in contingency table for {condition} to perform Fisher's Exact Test.")

    else:
        print(f"\n'{condition}' column not found in the dataset.")

mi_df_signi = mi_df.iloc[:-5]
mi_df_insigni = mi_df.iloc[-5:]

mi_df_insigni

"""### Model Training"""

# Extract features from mi_df_insigni
features_insigni = mi_df_insigni['Feature'].tolist()

# Subset the training data with these features
X_insigni = df_train[features_insigni]
y = df_train['Gallstone_Status']

# Add a constant to the predictor variables for logistic regression
X_insigni = sm.add_constant(X_insigni)

# Fit the logistic regression model using statsmodels
model_insigni = sm.Logit(y, X_insigni).fit()

# Print the summary of the logistic regression model
print(model_insigni.summary())

# Perform Likelihood Ratio Test

# Fit the restricted model (intercept-only)
X_restricted = sm.add_constant(pd.DataFrame({'Intercept': [1] * len(y)}, index=y.index))  # Ensure matching indices
model_restricted = sm.Logit(y, X_restricted).fit()

# Calculate the test statistic (LR = -2 * log-likelihood difference)
lr_stat = -2 * (model_restricted.llf - model_insigni.llf)

# Degrees of freedom = number of additional parameters in the full model
df_diff = len(features_insigni)

# Compute the p-value from the Chi-squared distribution
p_value = chi2.sf(lr_stat, df_diff)

# Determine significance
alpha = 0.05
if p_value < alpha:
    print(f"Likelihood Ratio Test: Significant (p-value={p_value:.12f})")
else:
    print(f"Likelihood Ratio Test: Not Significant (p-value={p_value:.12f})")

features_insigni_copy = ["Coronary Artery Disease (CAD)", "Body Mass Index (BMI)"]

# Subset the training data with these features
X_insigni = df_train[features_insigni_copy]
y = df_train['Gallstone_Status']

# Add a constant to the predictor variables for logistic regression
X_insigni = sm.add_constant(X_insigni)

# Fit the logistic regression model using statsmodels
model_insigni = sm.Logit(y, X_insigni).fit()

# Print the summary of the logistic regression model
print(model_insigni.summary())

# Perform Likelihood Ratio Test

# Fit the restricted model (intercept-only)
X_restricted = sm.add_constant(pd.DataFrame({'Intercept': [1] * len(y)}, index=y.index))  # Ensure matching indices
model_restricted = sm.Logit(y, X_restricted).fit()

# Calculate the test statistic (LR = -2 * log-likelihood difference)
lr_stat = -2 * (model_restricted.llf - model_insigni.llf)

# Degrees of freedom = number of additional parameters in the full model
df_diff = len(features_insigni_copy)

# Compute the p-value from the Chi-squared distribution
p_value = chi2.sf(lr_stat, df_diff)

# Determine significance
alpha = 0.05
if p_value < alpha:
    print(f"Likelihood Ratio Test: Significant (p-value={p_value:.12f})")
else:
    print(f"Likelihood Ratio Test: Not Significant (p-value={p_value:.12f})")

features_insigni_copy = [
    feature for feature in features_insigni
    if feature not in ["Coronary Artery Disease (CAD)", "Body Mass Index (BMI)"]
]

features_insigni_copy

# Subset the training data with these features
X_insigni = df_train[features_insigni_copy]
y = df_train['Gallstone_Status']

# Add a constant to the predictor variables for logistic regression
X_insigni = sm.add_constant(X_insigni)

# Fit the logistic regression model using statsmodels
model_insigni = sm.Logit(y, X_insigni).fit()

# Print the summary of the logistic regression model
print(model_insigni.summary())

# Perform Likelihood Ratio Test

# Fit the restricted model (intercept-only)
X_restricted = sm.add_constant(pd.DataFrame({'Intercept': [1] * len(y)}, index=y.index))  # Ensure matching indices
model_restricted = sm.Logit(y, X_restricted).fit()

# Calculate the test statistic (LR = -2 * log-likelihood difference)
lr_stat = -2 * (model_restricted.llf - model_insigni.llf)

# Degrees of freedom = number of additional parameters in the full model
df_diff = len(features_insigni_copy)

# Compute the p-value from the Chi-squared distribution
p_value = chi2.sf(lr_stat, df_diff)

# Determine significance
alpha = 0.05
if p_value < alpha:
    print(f"Likelihood Ratio Test: Significant (p-value={p_value:.12f})")
else:
    print(f"Likelihood Ratio Test: Not Significant (p-value={p_value:.12f})")

final_cols_list = mi_df_signi['Feature'].tolist() + features_insigni_copy

df_train_final = df_train[final_cols_list + ["Gallstone_Status"]].copy()

df_train_final

df_test = df_test[df_train_final.columns]

df_test

# Define the predictor variables (X) and the target variable (y)
X = df_train_final.drop(columns=["Gallstone_Status"])
y = df_train_final["Gallstone_Status"]

# Add a constant for the intercept
X = sm.add_constant(X)

# Fit the logistic regression model using statsmodels
model = sm.Logit(y, X).fit(maxiter=200)

# Display the regression results
print(model.summary())

# Calculate Nagelkerke R^2

ll_null = model.llnull  # Log-likelihood of the null model
ll_model = model.llf    # Log-likelihood of the fitted model
n = len(y)              # Number of observations
nagelkerke_r2 = (1 - np.exp((ll_null - ll_model) * 2 / n)) / (1 - np.exp(ll_null * 2 / n))
print(f"Nagelkerke R^2: {nagelkerke_r2:.4f}")

"""### Model Evaluation"""

from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score

# Define the predictor variables (X) and the target variable (y)
X = df_train_final.drop(columns=["Gallstone_Status"])
y = df_train_final["Gallstone_Status"]

# Create and fit the Logistic Regression model
model_Logistic_Regression = LogisticRegression(max_iter=5000, random_state=42)
model_Logistic_Regression.fit(X, y)

# Compute and print training accuracy
y_pred_train = model_Logistic_Regression.predict(X)
training_accuracy = accuracy_score(y, y_pred_train)
print(f"Training Accuracy: {training_accuracy:.4f}")

"""### Advanced Evaluation Metrics"""

from sklearn.metrics import roc_auc_score, roc_curve, ConfusionMatrixDisplay, confusion_matrix
import matplotlib.pyplot as plt

# Separate features and target in df_test
X_test = df_test.drop(columns=["Gallstone_Status"])
y_test = df_test["Gallstone_Status"]

# Predict probabilities and classes using the fitted Logistic Regression model
y_pred_proba = model_Logistic_Regression.predict_proba(X_test)[:, 1]  # Probabilities for the positive class
y_pred = model_Logistic_Regression.predict(X_test)  # Predicted classes

# Evaluate model performance
roc_auc = roc_auc_score(y_test, y_pred_proba)
cm = confusion_matrix(y_test, y_pred)

print(f"ROC AUC Score: {roc_auc:.4f}")
print("Confusion Matrix:")
print(cm)

# Plot ROC-AUC curve
fpr, tpr, thresholds = roc_curve(y_test, y_pred_proba)
plt.figure(figsize=(8, 6))
plt.plot(fpr, tpr, color='blue', label=f"ROC Curve (AUC = {roc_auc:.4f})")
plt.plot([0, 1], [0, 1], color='red', linestyle='--', label="Random Guess")
plt.xlabel("False Positive Rate (FPR)")
plt.ylabel("True Positive Rate (TPR)")
plt.title("ROC-AUC Curve")
plt.legend()
plt.grid()
plt.show()

# Plot Confusion Matrix
disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=model_Logistic_Regression.classes_)
disp.plot(cmap='Blues', values_format='d')
plt.title("Confusion Matrix")
plt.grid(False)
plt.show()

# Extract confusion matrix values
TN, FP, FN, TP = cm.ravel()  # Unpack True Negative, False Positive, False Negative, True Positive

# Calculate performance metrics
accuracy = (TP + TN) / (TP + TN + FP + FN)
precision = TP / (TP + FP) if (TP + FP) != 0 else 0  # Avoid division by zero
recall = TP / (TP + FN) if (TP + FN) != 0 else 0     # Avoid division by zero
f1_score = 2 * (precision * recall) / (precision + recall) if (precision + recall) != 0 else 0  # Avoid division by zero

# Print the results
print(f"Accuracy: {accuracy:.4f}")
print(f"Precision: {precision:.4f}")
print(f"Recall: {recall:.4f}")
print(f"F1-Score: {f1_score:.4f}")

from sklearn.tree import DecisionTreeClassifier
from sklearn.model_selection import GridSearchCV
from sklearn.metrics import accuracy_score, recall_score, precision_score, f1_score

# Separate features and target variable from df_train_final
X_train = df_train_final.drop(columns=["Gallstone_Status"])  # Features
y_train = df_train_final["Gallstone_Status"]  # Target variable

# Initialize DecisionTreeClassifier
decision_tree = DecisionTreeClassifier(random_state=42)

# Define the parameter grid for min_samples_split, max_features, and min_impurity_decrease
param_grid = {
    'min_samples_split': list(range(2, 256)),
    'max_depth': list(range(1, 27))
}

# Configure GridSearchCV to maximize accuracy with 10-fold CV
grid_search = GridSearchCV(
    estimator=decision_tree,
    param_grid=param_grid,
    scoring='accuracy',  # Optimize for accuracy
    cv=10,  # 10-fold Cross Validation
    n_jobs=-1,  # Use all processors
    verbose=1  # Print progress
)

# Perform the grid search
grid_search.fit(X_train, y_train)

# Get the best model
model_Decision_Tree = grid_search.best_estimator_

# Fit the best model to the training data
model_Decision_Tree.fit(X_train, y_train)

# Make predictions on the training set
y_train_pred = model_Decision_Tree.predict(X_train)

# Calculate training metrics
train_accuracy = accuracy_score(y_train, y_train_pred)
train_recall = recall_score(y_train, y_train_pred)
train_precision = precision_score(y_train, y_train_pred)
train_f1 = f1_score(y_train, y_train_pred)

# Print the results
print(f"Optimal max_depth: {grid_search.best_params_['max_depth']}")
print(f"Optimal min_samples_split: {grid_search.best_params_['min_samples_split']}")
print(f"Training Accuracy: {train_accuracy:.4f}")
print(f"Training Recall: {train_recall:.4f}")
print(f"Training Precision: {train_precision:.4f}")
print(f"Training F1 Score: {train_f1:.4f}")

from sklearn.tree import plot_tree
import matplotlib.pyplot as plt

# Plot the decision tree
plt.figure(figsize=(20, 10))  # Adjust the size of the plot
plot_tree(
    model_Decision_Tree,  # Fitted decision tree model
    feature_names=X_train.columns,  # Features (columns from X_train)
    class_names=[str(c) for c in model_Decision_Tree.classes_],  # Class names
    filled=True,  # Fill the nodes with colors based on the class
    rounded=True,  # Rounded boxes for better aesthetics
    proportion=True,  # Scale nodes based on samples proportion
    fontsize=10  # Adjust font size for better readability
)

plt.title("Decision Tree Visualization", fontsize=16)
plt.show()

from sklearn.metrics import roc_curve, roc_auc_score, ConfusionMatrixDisplay, accuracy_score, precision_score, recall_score, f1_score
import matplotlib.pyplot as plt
import numpy as np

# Extract features and target from the test set
X_test = df_test.drop(columns=["Gallstone_Status"])  # Features
y_test = df_test["Gallstone_Status"]  # Target column

# Predict probabilities for the positive class
y_pred_proba = model_Decision_Tree.predict_proba(X_test)[:, 1]  # Probabilities for the positive class

# Predict class labels
y_pred = model_Decision_Tree.predict(X_test)

# Plot the ROC-AUC curve
fpr, tpr, thresholds = roc_curve(y_test, y_pred_proba)

plt.figure(figsize=(10, 6))
plt.plot(fpr, tpr, color="blue", label=f"ROC Curve (AUC = {roc_auc:.4f})")
plt.plot([0, 1], [0, 1], color="red", linestyle="--", label="Random Classifier (AUC = 0.5)")
plt.xlabel("False Positive Rate (FPR)")
plt.ylabel("True Positive Rate (Recall)")
plt.title("ROC-AUC Curve")
plt.legend(loc="best")
plt.grid()
plt.show()

# Plot the confusion matrix
ConfusionMatrixDisplay.from_estimator(
    model_Decision_Tree,
    X_test,
    y_test,
    cmap="Blues",
    values_format="d"
)
plt.title("Confusion Matrix")
plt.grid(False)  # Remove grid lines
plt.show()

# Calculate performance metrics
accuracy = accuracy_score(y_test, y_pred)
precision = precision_score(y_test, y_pred)
recall = recall_score(y_test, y_pred)
f1 = f1_score(y_test, y_pred)
roc_auc = roc_auc_score(y_test, y_pred_proba)

# Print performance metrics
print(f"Test Accuracy: {accuracy:.4f}")
print(f"Test Precision: {precision:.4f}")
print(f"Test Recall: {recall:.4f}")
print(f"Test F1 Score: {f1:.4f}")
print(f"Test ROC-AUC: {roc_auc:.4f}")

from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import GridSearchCV

# Separate features and target variable from df_train_final
X_train = df_train_final.drop(columns=["Gallstone_Status"])  # Features
y_train = df_train_final["Gallstone_Status"]  # Target variable

# Initialize Random Forest Classifier with required parameters
random_forest = RandomForestClassifier(
    n_jobs=-1,
    random_state=42,
    class_weight='balanced'
)

param_grid = {
    'n_estimators': [1200, 2400, 3600, 4800],                   # Number of trees in the forest
    'min_samples_split': [2, 5, 10],                  # Minimum samples required to split a node
    'max_features': ['sqrt', 'log2'],               # Number of features considered at each split
    'max_samples': [0.5, 0.75, None]                # Fraction of the training data used in bootstrap
}

# Set up GridSearchCV
grid_search_rf = GridSearchCV(
    estimator=random_forest,
    param_grid=param_grid,
    scoring='accuracy',      # Optimize for accuracy
    cv=10,                   # 10-fold Cross-Validation
    n_jobs=-1,               # Parallel computing
    verbose=1                # Print progress
)

# Perform Grid Search
grid_search_rf.fit(X_train, y_train)

# Store the best GridSearchCV RF model in model_Random_Forest
model_Random_Forest = grid_search_rf.best_estimator_

# Get the best parameters
best_rf_params = grid_search_rf.best_params_

# Print the optimal hyperparameters
print(f"Best Parameters: {best_rf_params}")

# Fit the best model to the training data
y_train_pred = model_Random_Forest.predict(X_train)
y_train_pred_proba = model_Random_Forest.predict_proba(X_train)[:, 1]  # Get probabilities for ROC-AUC

# Compute performance metrics on the training set
train_accuracy = accuracy_score(y_train, y_train_pred)
train_precision = precision_score(y_train, y_train_pred)
train_recall = recall_score(y_train, y_train_pred)
train_f1 = f1_score(y_train, y_train_pred)
train_auc_roc = roc_auc_score(y_train, y_train_pred_proba)

# Print the performance metrics
print(f"Training Accuracy: {train_accuracy:.4f}")
print(f"Training Precision: {train_precision:.4f}")
print(f"Training Recall: {train_recall:.4f}")
print(f"Training F1 Score: {train_f1:.4f}")
print(f"Training ROC-AUC: {train_auc_roc:.4f}")

from sklearn.metrics import roc_curve, roc_auc_score, ConfusionMatrixDisplay, accuracy_score, precision_score, recall_score, f1_score
import matplotlib.pyplot as plt

# Extract features and target from the test set
X_test = df_test.drop(columns=["Gallstone_Status"])  # Features
y_test = df_test["Gallstone_Status"]  # Target variable

# Predict probabilities for the positive class
y_pred_proba = model_Random_Forest.predict_proba(X_test)[:, 1]  # Probabilities for the positive class

# Predict class labels
y_pred = model_Random_Forest.predict(X_test)

# Plot the ROC-AUC curve
fpr, tpr, thresholds = roc_curve(y_test, y_pred_proba)

plt.figure(figsize=(10, 6))
plt.plot(fpr, tpr, color="blue", label=f"ROC Curve (AUC = {roc_auc:.4f})")
plt.plot([0, 1], [0, 1], color="red", linestyle="--", label="Random Classifier (AUC = 0.5)")
plt.xlabel("False Positive Rate (FPR)")
plt.ylabel("True Positive Rate (Recall)")
plt.title("ROC-AUC Curve")
plt.legend(loc="best")
plt.grid()
plt.show()

# Plot the confusion matrix
ConfusionMatrixDisplay.from_estimator(
    model_Random_Forest,
    X_test,
    y_test,
    cmap="Blues",
    values_format="d"
)
plt.title("Confusion Matrix")
plt.grid(False)  # Remove grid lines
plt.show()

# Calculate performance metrics
accuracy = accuracy_score(y_test, y_pred)
precision = precision_score(y_test, y_pred)
recall = recall_score(y_test, y_pred)
f1 = f1_score(y_test, y_pred)
roc_auc = roc_auc_score(y_test, y_pred_proba)

# Print performance metrics
print(f"Test Accuracy: {accuracy:.4f}")
print(f"Test Precision: {precision:.4f}")
print(f"Test Recall: {recall:.4f}")
print(f"Test F1 Score: {f1:.4f}")
print(f"Test ROC-AUC: {roc_auc:.4f}")

# Get feature importance from model_Random_Forest
feature_importances_rf = model_Random_Forest.feature_importances_

# Extract feature names (assuming features are from the test set's columns)
feature_names_rf = X_test.columns

# Create a DataFrame for better visualization and sorting
importance_df_rf = pd.DataFrame({
    'Feature': feature_names_rf,
    'Importance': feature_importances_rf
})

# Sort the features based on importance values in descending order
importance_df_rf = importance_df_rf.sort_values(by='Importance', ascending=False)

# Plot the feature importance bar graph
plt.figure(figsize=(10, 8))
plt.barh(importance_df_rf['Feature'], importance_df_rf['Importance'], color='lightgreen')
plt.xlabel('Feature Importance', fontsize=14)
plt.ylabel('Features', fontsize=14)
plt.title('Feature Importance - Random Forest Model', fontsize=16)
plt.gca().invert_yaxis()  # Invert y-axis so the most important feature is on top
plt.grid(axis='x', linestyle='--', alpha=0.6)
plt.tight_layout()
plt.show()

import warnings
warnings.filterwarnings("ignore")

from xgboost import XGBClassifier
from sklearn.model_selection import GridSearchCV

# Extract features and target from the training set
X_train = df_train_final.drop(columns=["Gallstone_Status"])  # Features
y_train = df_train_final["Gallstone_Status"]  # Target variable

# Initialize the XGBoost Classifier with required base parameters
xgb_classifier = XGBClassifier(
    verbosity=1,
    n_jobs=-1,
    random_state=42,
    use_label_encoder=False,  # Avoids unnecessary warnings in XGBoost
    eval_metric="logloss"    # Ensures compatibility with binary classification
)

# Define the parameter grid for XGBoost
param_grid_xgb = {
    'n_estimators': [400, 800, 1600, 2400],  # Number of boosting rounds
    'booster': ['dart'],                          # Booster type: tree-based or Dropouts meet Multiple Additive Regression Trees
    'eta': [0.01, 0.1, 0.2, 0.4],            # Learning rate
    'alpha': [0, 1, 5]                        # L1 regularization (controls overfitting)
}

# Set up GridSearchCV
grid_search_xgb = GridSearchCV(
    estimator=xgb_classifier,
    param_grid=param_grid_xgb,
    scoring='accuracy',      # Optimize for accuracy
    cv=10,                   # 10-fold Cross-Validation
    n_jobs=-1,               # Use all available cores
    verbose=2                # Verbosity level
)

# Perform GridSearchCV to find the best parameters and fit the training dataset
grid_search_xgb.fit(X_train, y_train)

# Get the best model and parameters
model_XGBoost = grid_search_xgb.best_estimator_
best_xgb_params = grid_search_xgb.best_params_

# Print the optimal hyperparameters
print(f"Best Parameters: {best_xgb_params}")

# Fit the best model to the training data
y_train_pred = model_XGBoost.predict(X_train)
y_train_pred_proba = model_XGBoost.predict_proba(X_train)[:, 1]  # Predicted probabilities for positive class

# Compute performance metrics on the training set
train_accuracy = accuracy_score(y_train, y_train_pred)
train_precision = precision_score(y_train, y_train_pred)
train_recall = recall_score(y_train, y_train_pred)
train_f1 = f1_score(y_train, y_train_pred)
train_auc_roc = roc_auc_score(y_train, y_train_pred_proba)

# Print the performance metrics
print(f"Training Accuracy: {train_accuracy:.4f}")
print(f"Training Precision: {train_precision:.4f}")
print(f"Training Recall: {train_recall:.4f}")
print(f"Training F1 Score: {train_f1:.4f}")
print(f"Training ROC-AUC: {train_auc_roc:.4f}") 0.4, 'n_estimators': 400}

# Extract features and target from the test set
X_test = df_test.drop(columns=["Gallstone_Status"])  # Features
y_test = df_test["Gallstone_Status"]  # Target variable

# Predict probabilities for the positive class
y_test_pred_proba = model_XGBoost.predict_proba(X_test)[:, 1]  # Probabilities for the positive class

# Predict class labels
y_test_pred = model_XGBoost.predict(X_test)

# Calculate performance metrics
test_accuracy = accuracy_score(y_test, y_test_pred)
test_precision = precision_score(y_test, y_test_pred)
test_recall = recall_score(y_test, y_test_pred)
test_f1 = f1_score(y_test, y_test_pred)
test_roc_auc = roc_auc_score(y_test, y_test_pred_proba)

# Print performance metrics
print(f"Test Accuracy: {test_accuracy:.4f}")
print(f"Test Precision: {test_precision:.4f}")
print(f"Test Recall: {test_recall:.4f}")
print(f"Test F1 Score: {test_f1:.4f}")
print(f"Test ROC-AUC: {test_roc_auc:.4f}")

# Plot the ROC-AUC curve
fpr, tpr, thresholds = roc_curve(y_test, y_test_pred_proba)

plt.figure(figsize=(10, 6))
plt.plot(fpr, tpr, color="blue", label=f"ROC Curve (AUC = {test_roc_auc:.4f})")
plt.plot([0, 1], [0, 1], color="red", linestyle="--", label="Random Classifier (AUC = 0.5)")
plt.xlabel("False Positive Rate (FPR)")
plt.ylabel("True Positive Rate (Recall)")
plt.title("ROC-AUC Curve")
plt.legend(loc="best")
plt.grid()
plt.show()

# Plot the confusion matrix
ConfusionMatrixDisplay.from_estimator(
    model_XGBoost,
    X_test,
    y_test,
    cmap="Blues",
    values_format="d"
)
plt.title("Confusion Matrix")
plt.grid(False)  # Remove grid lines
plt.show()

# Get feature importance from model_XGBoost
feature_importances = model_XGBoost.feature_importances_

# Extract feature names (assuming features are from the test set's columns)
feature_names = X_test.columns

# Create a DataFrame for better visualization and sorting
importance_df = pd.DataFrame({
    'Feature': feature_names,
    'Importance': feature_importances
})

# Sort the features based on importance values in descending order
importance_df = importance_df.sort_values(by='Importance', ascending=False)

# Plot the feature importance bar graph
plt.figure(figsize=(10, 8))
plt.barh(importance_df['Feature'], importance_df['Importance'], color='skyblue')
plt.xlabel('Feature Importance', fontsize=14)
plt.ylabel('Features', fontsize=14)
plt.title('Feature Importance - XGBoost Model', fontsize=16)
plt.gca().invert_yaxis()  # Invert y-axis so the most important feature is on top
plt.grid(axis='x', linestyle='--', alpha=0.6)
plt.tight_layout()
plt.show()

# Define metrics for each model (replace these with actual values from your results)
metrics = ['Accuracy', 'Precision', 'Recall', 'F1 Score', 'AUC-ROC']

# Example data for the models (in percentages; update with your actual values)
logistic_regression = [79.69, 76.32, 87.88, 81.69, 85.24]  # Replace with actual results
decision_tree = [75.00, 75.76, 75.76, 75.76, 80.16]        # Replace with actual results
random_forest = [81.25, 78.38, 87.88, 82.86, 88.95]        # Replace with actual results
xgboost = [85.94, 87.50, 84.85, 86.15, 88.17]             # Replace with actual results

# Combine all data for easy plotting
model_metrics = np.array([logistic_regression, decision_tree, random_forest, xgboost])

# Model names for the x-axis
models = ['Logistic Regression', 'Decision Tree', 'Random Forest', 'XGBoost']

# Number of metrics
num_metrics = len(metrics)

# Bar width
bar_width = 0.2

# Positions for the bars for each model
x = np.arange(len(metrics))
positions_logistic = x - bar_width * 1.5
positions_decision_tree = x - bar_width * 0.5
positions_random_forest = x + bar_width * 0.5
positions_xgboost = x + bar_width * 1.5

# Plotting the side-by-side bars
plt.figure(figsize=(12, 6))
plt.bar(positions_logistic, logistic_regression, width=bar_width, label='Logistic Regression', color='blue', alpha=0.8)
plt.bar(positions_decision_tree, decision_tree, width=bar_width, label='Decision Tree', color='green', alpha=0.8)
plt.bar(positions_random_forest, random_forest, width=bar_width, label='Random Forest', color='orange', alpha=0.8)
plt.bar(positions_xgboost, xgboost, width=bar_width, label='XGBoost', color='red', alpha=0.8)

# Adding labels and titles
plt.xlabel('Metrics', fontsize=14)
plt.ylabel('Performance (%)', fontsize=14)
plt.title('Model Performance Comparison', fontsize=16)
plt.xticks(x, metrics, fontsize=12)
plt.yticks(fontsize=12)
plt.legend(fontsize=12, loc='lower left')
plt.grid(axis='y', linestyle='--', alpha=0.6)

# Show the plot
plt.tight_layout()
plt.show()